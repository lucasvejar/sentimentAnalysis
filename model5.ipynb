{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit83e830d1b6064148ae61d1aa05c0276d",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pathlib\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the dataset \n",
    "data_dir = pathlib.Path('/home/lv11/Documents/ProyectosPython/sentimentAnalysis/train')\n",
    "nf = pd.read_csv(data_dir / 'tweetsDataset1.csv',skiprows=1,names=['Message','Target'])\n",
    "#print(nf.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "stop_words = list(STOP_WORDS)\n",
    "\n",
    "messages = nf['Message']\n",
    "labels = nf['Target']\n",
    "#print(stop_words)\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    tokens = nlp(sentence)\n",
    "    tokens = [ word.lemma_.lower().strip() if word.lemma_ != '-PRON-' else word.lower_ for word in tokens ]\n",
    "    tokens = [ word for word in tokens if word not in stop_words and word not in punctuation ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(tweets):\n",
    "    vector = CountVectorizer(tokenizer=spacy_tokenizer)\n",
    "    frequency_matrix = vector.fit_transform(tweets)\n",
    "    sum_frequencies = np.sum(frequency_matrix,axis=0)\n",
    "    frequency = np.squeeze(np.asarray(sum_frequencies))\n",
    "    frequency_dataFrame = pd.DataFrame([frequency],columns=vector.get_feature_names()).transpose()\n",
    "    return frequency_dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(messages,labels):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(messages,labels,test_size=0.25,random_state=42,shuffle=True)\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization_tweets(dataset,features):\n",
    "    tokenization = TfidfVectorizer(max_features=features)\n",
    "    tokenization.fit(dataset)\n",
    "    dataset_transformed = tokenization.transform(dataset).toarray()\n",
    "    return dataset_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train_mod, y_train, features, shuffle, drop, layer1, layer2, epoch, lr, epsilon, validation):\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(layer1, input_shape=(features,), activation='relu'))\n",
    "    model_nn.add(Dropout(drop))\n",
    "    model_nn.add(Dense(layer2, activation='sigmoid'))\n",
    "    model_nn.add(Dropout(drop))\n",
    "    model_nn.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=epsilon,decay=0.0, amsgrad=False)\n",
    "    model_nn.compile(loss='sparse_categorical_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    model_nn.fit(np.array(X_train_mod), y_train,\n",
    "                 batch_size=32,\n",
    "                 epochs=epoch,\n",
    "                 verbose=1,\n",
    "                 validation_split=validation,\n",
    "                 shuffle=True)\n",
    "    return model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test,model):\n",
    "    predict = model.predict(X_test)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo1(X_train,y_train):\n",
    "    features = 1554 #1727 # puede ser 3727 o un valor asi que vi como len() del array de X_train \n",
    "    shuffle = True\n",
    "    drop = 0.5\n",
    "    layer1 = 512\n",
    "    layer2 = 256\n",
    "    epoch = 10\n",
    "    lr = 0.002\n",
    "    epsilon = 1e-9\n",
    "    validation = 0.1\n",
    "    X_train_mod = tokenization_tweets(X_train,features)\n",
    "    modelo = train(X_train_mod, y_train,features,shuffle,drop,layer1,layer2,epoch,lr,epsilon,validation)\n",
    "    return modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dataFrame = vectorization(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = splitting(messages,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 1554 samples, validate on 173 samples\nEpoch 1/10\n1554/1554 [==============================] - 1s 915us/step - loss: 0.6877 - accuracy: 0.6197 - val_loss: 0.3283 - val_accuracy: 0.8728\nEpoch 2/10\n1554/1554 [==============================] - 1s 692us/step - loss: 0.2361 - accuracy: 0.9048 - val_loss: 0.2067 - val_accuracy: 0.8960\nEpoch 3/10\n1554/1554 [==============================] - 1s 801us/step - loss: 0.1199 - accuracy: 0.9582 - val_loss: 0.2791 - val_accuracy: 0.8728\nEpoch 4/10\n1554/1554 [==============================] - 1s 784us/step - loss: 0.0721 - accuracy: 0.9730 - val_loss: 0.3040 - val_accuracy: 0.8786\nEpoch 5/10\n1554/1554 [==============================] - 1s 765us/step - loss: 0.0586 - accuracy: 0.9794 - val_loss: 0.3154 - val_accuracy: 0.8555\nEpoch 6/10\n1554/1554 [==============================] - 1s 731us/step - loss: 0.0379 - accuracy: 0.9858 - val_loss: 0.3839 - val_accuracy: 0.8555\nEpoch 7/10\n1554/1554 [==============================] - 1s 784us/step - loss: 0.0261 - accuracy: 0.9903 - val_loss: 0.4714 - val_accuracy: 0.8324\nEpoch 8/10\n1554/1554 [==============================] - 1s 781us/step - loss: 0.0257 - accuracy: 0.9903 - val_loss: 0.4614 - val_accuracy: 0.8439\nEpoch 9/10\n1554/1554 [==============================] - 1s 760us/step - loss: 0.0189 - accuracy: 0.9916 - val_loss: 0.5021 - val_accuracy: 0.8324\nEpoch 10/10\n1554/1554 [==============================] - 1s 749us/step - loss: 0.0235 - accuracy: 0.9903 - val_loss: 0.5652 - val_accuracy: 0.8324\n"
    }
   ],
   "source": [
    "modelo = modelo1(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_46_input to have shape (1554,) but got array with shape (1,)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-ad0a6df8096f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/conda-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-env/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_46_input to have shape (1554,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "pred = modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}